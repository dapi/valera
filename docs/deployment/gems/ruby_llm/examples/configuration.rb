#!/usr/bin/env ruby

require 'ruby_llm'
require 'dotenv/load'

# –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π Ruby LLM
class RubyLLMConfigurationExamples
  def self.run_all_examples
    puts "‚öôÔ∏è –ü—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Ruby LLM"
    puts "=" * 50

    demonstrate_basic_configuration
    demonstrate_multi_provider_configuration
    demonstrate_environment_specific_config
    demonstrate_advanced_configuration
    demonstrate_error_handling_config
    demonstrate_rails_integration_config
  end

  private

  def self.demonstrate_basic_configuration
    puts "\nüìã –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
    puts "-" * 35

    config_code = <<~RUBY
      # config/initializers/ruby_llm.rb
      require 'ruby_llm'

      RubyLLM.configure do |config|
        # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config.use_new_acts_as = true
        config.request_timeout = 120
        config.max_retries = 2

        # API –∫–ª—é—á OpenAI
        config.openai_api_key = ENV.fetch('OPENAI_API_KEY', nil)

        # –ú–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        config.default_model = 'gpt-3.5-turbo'
        config.default_embedding_model = 'text-embedding-3-large'
        config.default_image_model = 'dall-e-3'
      end
    RUBY

    puts config_code

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    configure_basic
    puts "\n‚úÖ –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
    puts "   –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: #{RubyLLM.configuration.default_model}"
  end

  def self.demonstrate_multi_provider_configuration
    puts "\nüåê –ü—Ä–∏–º–µ—Ä 2: –ú—É–ª—å—Ç–∏-–ø—Ä–æ–≤–∞–π–¥–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
    puts "-" * 40

    config_code = <<~RUBY
      # config/initializers/ruby_llm.rb
      require 'ruby_llm'

      RubyLLM.configure do |config|
        # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config.use_new_acts_as = true
        config.request_timeout = 120
        config.max_retries = 3

        # OpenAI
        config.openai_api_key = ENV.fetch('OPENAI_API_KEY', nil)

        # Anthropic Claude
        config.anthropic_api_key = ENV.fetch('ANTHROPIC_API_KEY', nil)

        # Google Gemini
        config.gemini_api_key = ENV.fetch('GEMINI_API_KEY', nil)

        # DeepSeek
        config.deepseek_api_key = ENV.fetch('DEEPSEEK_API_KEY', nil)

        # Mistral
        config.mistral_api_key = ENV.fetch('MISTRAL_API_KEY', nil)

        # OpenRouter (–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –º–æ–¥–µ–ª–µ–π)
        config.openrouter_api_key = ENV.fetch('OPENROUTER_API_KEY', nil)

        # –ú–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
        config.default_model = 'gpt-3.5-turbo'           # –î–ª—è —á–∞—Ç–æ–≤
        config.default_embedding_model = 'text-embedding-3-large'  # –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        config.default_image_model = 'dall-e-3'          # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
      end
    RUBY

    puts config_code

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
    configure_multi_provider
    puts "\n‚úÖ –ú—É–ª—å—Ç–∏-–ø—Ä–æ–≤–∞–π–¥–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
    puts "   –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã: OpenAI, Anthropic, Google Gemini, DeepSeek, Mistral, OpenRouter"
  end

  def self.demonstrate_environment_specific_config
    puts "\nüèóÔ∏è –ü—Ä–∏–º–µ—Ä 3: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π"
    puts "-" * 42

    config_code = <<~RUBY
      # config/initializers/ruby_llm.rb
      require 'ruby_llm'

      # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
      base_config = {
        use_new_acts_as: true,
        request_timeout: 120,
        max_retries: 2
      }

      # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
      development_config = base_config.merge(
        request_timeout: 30,
        max_retries: 1,
        default_model: 'gpt-3.5-turbo'  # –ë–æ–ª–µ–µ –¥–µ—à–µ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
      )

      # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
      production_config = base_config.merge(
        request_timeout: 180,
        max_retries: 5,
        default_model: 'gpt-4'  # –ë–æ–ª–µ–µ –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
      )

      # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
      test_config = base_config.merge(
        request_timeout: 10,
        max_retries: 0,
        default_model: 'gpt-3.5-turbo'
      )

      # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏—è
      config_to_use = case Rails.env
                      when 'development'
                        development_config
                      when 'production'
                        production_config
                      when 'test'
                        test_config
                      else
                        base_config
                      end

      RubyLLM.configure do |config|
        config_to_use.each { |key, value| config.send("#{key}=", value) }

        # API –∫–ª—é—á–∏ (–∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ ENV)
        config.openai_api_key = ENV.fetch('OPENAI_API_KEY', nil)
        config.anthropic_api_key = ENV.fetch('ANTHROPIC_API_KEY', nil)
        config.gemini_api_key = ENV.fetch('GEMINI_API_KEY', nil)
      end
    RUBY

    puts config_code

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    configure_for_environment('development')
    puts "\n‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è development –æ–∫—Ä—É–∂–µ–Ω–∏—è"
    puts "   –¢–∞–π–º–∞—É—Ç: #{RubyLLM.configuration.request_timeout}s"
    puts "   –ü–æ–≤—Ç–æ—Ä–æ–≤: #{RubyLLM.configuration.max_retries}"
    puts "   –ú–æ–¥–µ–ª—å: #{RubyLLM.configuration.default_model}"
  end

  def self.demonstrate_advanced_configuration
    puts "\nüöÄ –ü—Ä–∏–º–µ—Ä 4: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å middleware"
    puts "-" * 48

    config_code = <<~RUBY
      # config/initializers/ruby_llm.rb
      require 'ruby_llm'

      class LLMLogger
        def self.log_request(request)
          Rails.logger.info "LLM Request: #{request.model} - #{request.messages&.last&.content&.truncate(50)}"
        end

        def self.log_response(response)
          Rails.logger.info "LLM Response: #{response.usage[:total_tokens]} tokens"
        end
      end

      class LLMRateLimiter
        @requests = {}
        @mutex = Mutex.new

        def self.check_limit(user_id)
          @mutex.synchronize do
            now = Time.now
            @requests[user_id] ||= []
            @requests[user_id] = @requests[user_id].select { |t| now - t < 60 }  # –ó–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –º–∏–Ω—É—Ç—É

            if @requests[user_id].size >= 10  # –ú–∞–∫—Å–∏–º—É–º 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É
              raise "Rate limit exceeded for user #{user_id}"
            end

            @requests[user_id] << now
          end
        end
      end

      RubyLLM.configure do |config|
        # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config.use_new_acts_as = true
        config.request_timeout = 120
        config.max_retries = 3

        # API –∫–ª—é—á–∏
        config.openai_api_key = ENV.fetch('OPENAI_API_KEY', nil)
        config.anthropic_api_key = ENV.fetch('ANTHROPIC_API_KEY', nil)

        # –ú–æ–¥–µ–ª–∏
        config.default_model = 'gpt-3.5-turbo'
        config.default_embedding_model = 'text-embedding-3-large'
        config.default_image_model = 'dall-e-3'

        # Middleware/hooks
        config.before_request = lambda do |request|
          user_id = request.metadata[:user_id]
          LLMRateLimiter.check_limit(user_id) if user_id
          LLMLogger.log_request(request)
        end

        config.after_request = lambda do |response|
          LLMLogger.log_response(response)
        end

        config.on_error = lambda do |error, request|
          Rails.logger.error "LLM Error: #{error.message} for request #{request.id}"
          # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ
          ErrorNotifier.notify(error, context: { request: request })
        end
      end
    RUBY

    puts config_code

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    configure_with_middleware
    puts "\n‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å middleware —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
    puts "   –í–∫–ª—é—á–µ–Ω—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, rate limiting –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫"
  end

  def self.demonstrate_error_handling_config
    puts "\nüõ°Ô∏è –ü—Ä–∏–º–µ—Ä 5: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"
    puts "-" * 40

    config_code = <<~RUBY
      # config/initializers/ruby_llm.rb
      require 'ruby_llm'

      class ResilientLLMClient
        MAX_RETRIES = 3
        BASE_DELAY = 1
        MAX_DELAY = 60

        def self.chat_with_fallback(message, model: nil, system: nil)
          models_to_try = model ? [model] : [
            'gpt-3.5-turbo',
            'claude-haiku-3.5',
            'gemini-pro'
          ]

          models_to_try.each do |current_model|
            begin
              return attempt_chat_with_retry(message, model: current_model, system: system)
            rescue RubyLLM::AuthenticationError => e
              Rails.logger.error "Authentication failed for #{current_model}: #{e.message}"
              next  # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
            rescue RubyLLM::RateLimitError => e
              Rails.logger.warn "Rate limit for #{current_model}, retrying..."
              sleep(e.retry_after || BASE_DELAY)
              retry
            rescue RubyLLM::APIError => e
              Rails.logger.error "API error for #{current_model}: #{e.message}"
              next  # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
            rescue => e
              Rails.logger.error "Unexpected error with #{current_model}: #{e.message}"
              next
            end
          end

          raise "All models failed to process the request"
        end

        private

        def self.attempt_chat_with_retry(message, model:, system:)
          attempts = 0

          begin
            chat = RubyLLM.chat.new(model: model, system: system)
            chat.say(message)
          rescue RubyLLM::RateLimitError, RubyLLM::APIError => e
            if attempts < MAX_RETRIES
              attempts += 1
              delay = [BASE_DELAY * (2 ** (attempts - 1)), MAX_DELAY].min
              Rails.logger.info "Retrying in #{delay}s (attempt #{attempts}/#{MAX_RETRIES})"
              sleep(delay)
              retry
            else
              raise e
            end
          end
        end
      end

      RubyLLM.configure do |config|
        config.use_new_acts_as = true
        config.request_timeout = 60
        config.max_retries = 2
        config.openai_api_key = ENV.fetch('OPENAI_API_KEY', nil)
        config.anthropic_api_key = ENV.fetch('ANTHROPIC_API_KEY', nil)
        config.gemini_api_key = ENV.fetch('GEMINI_API_KEY', nil)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
        config.on_error = lambda do |error, request|
          case error
          when RubyLLM::AuthenticationError
            ErrorNotifier.auth_error(error)
          when RubyLLM::RateLimitError
            ErrorNotifier.rate_limit_error(error, request.metadata[:user_id])
          when RubyLLM::APIError
            ErrorNotifier.api_error(error)
          else
            ErrorNotifier.general_error(error)
          end
        end
      end
    RUBY

    puts config_code

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    configure_with_error_handling
    puts "\n‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
    puts "   –í–∫–ª—é—á–µ–Ω—ã retry –ª–æ–≥–∏–∫–∞ –∏ fallback –º–æ–¥–µ–ª–∏"
  end

  def self.demonstrate_rails_integration_config
    puts "\nüîß –ü—Ä–∏–º–µ—Ä 6: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Rails –∏ anyway_config"
    puts "-" * 48

    config_code = <<~RUBY
      # config/configs/application_config.rb
      class ApplicationConfig
        config_name :application

        # LLM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        attr_config :llm_model, default: 'gpt-3.5-turbo'
        attr_config :llm_temperature, default: 0.7
        attr_config :llm_max_tokens, default: 1000
        attr_config :llm_request_timeout, default: 120
        attr_config :llm_max_retries, default: 2
        attr_config :llm_system_prompt, default: 'You are a helpful assistant.'

        # API –∫–ª—é—á–∏ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ ENV)
        attr_config :openai_api_key, default: nil
        attr_config :anthropic_api_key, default: nil
        attr_config :gemini_api_key, default: nil

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        required :llm_model

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
        validates :llm_temperature, inclusion: { in: 0.0..2.0 }
        validates :llm_max_tokens, inclusion: { in: 1..4096 }
        validates :llm_request_timeout, inclusion: { in: 10..300 }
        validates :llm_max_retries, inclusion: { in: 0..5 }
      end

      # config/initializers/ruby_llm.rb
      require 'ruby_llm'

      RubyLLM.configure do |config|
        # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config.use_new_acts_as = true
        config.request_timeout = ApplicationConfig.llm_request_timeout
        config.max_retries = ApplicationConfig.llm_max_retries

        # API –∫–ª—é—á–∏
        config.openai_api_key = ApplicationConfig.openai_api_key || ENV.fetch('OPENAI_API_KEY', nil)
        config.anthropic_api_key = ApplicationConfig.anthropic_api_key || ENV.fetch('ANTHROPIC_API_KEY', nil)
        config.gemini_api_key = ApplicationConfig.gemini_api_key || ENV.fetch('GEMINI_API_KEY', nil)

        # –ú–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        config.default_model = ApplicationConfig.llm_model
        config.default_embedding_model = 'text-embedding-3-large'
        config.default_image_model = 'dall-e-3'

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Rails –æ–∫—Ä—É–∂–µ–Ω–∏—è
        if Rails.env.development?
          config.default_model = 'gpt-3.5-turbo'  # –ë–æ–ª–µ–µ –¥–µ—à–µ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
        elsif Rails.env.production?
          config.default_model = ApplicationConfig.llm_model
        end
      end

      # app/services/llm_service.rb
      class LLMService
        def self.chat(message, options = {})
          model = options[:model] || ApplicationConfig.llm_model
          system = options[:system] || ApplicationConfig.llm_system_prompt
          temperature = options[:temperature] || ApplicationConfig.llm_temperature
          max_tokens = options[:max_tokens] || ApplicationConfig.llm_max_tokens

          chat = RubyLLM.chat.new(
            model: model,
            system: system,
            temperature: temperature,
            max_tokens: max_tokens
          )

          chat.say(message)
        end

        def self.embed(text)
          RubyLLM.embed(text, model: 'text-embedding-3-large')
        end

        def self.generate_image(prompt, options = {})
          model = options[:model] || 'dall-e-3'
          size = options[:size] || '1024x1024'
          quality = options[:quality] || 'standard'

          RubyLLM.paint(prompt, model: model, size: size, quality: quality)
        end
      end
    RUBY

    puts config_code

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    configure_rails_integration
    puts "\n‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å Rails –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
    puts "   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è anyway_config –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"
  end

  # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏

  def self.configure_basic
    RubyLLM.configure do |config|
      config.use_new_acts_as = true
      config.request_timeout = 120
      config.max_retries = 2
      config.openai_api_key = ENV.fetch('OPENAI_API_KEY', 'demo-key')
      config.default_model = 'gpt-3.5-turbo'
      config.default_embedding_model = 'text-embedding-3-large'
      config.default_image_model = 'dall-e-3'
    end
  end

  def self.configure_multi_provider
    RubyLLM.configure do |config|
      config.use_new_acts_as = true
      config.request_timeout = 120
      config.max_retries = 3
      config.openai_api_key = ENV.fetch('OPENAI_API_KEY', 'demo-key')
      config.anthropic_api_key = ENV.fetch('ANTHROPIC_API_KEY', 'demo-key')
      config.gemini_api_key = ENV.fetch('GEMINI_API_KEY', 'demo-key')
      config.deepseek_api_key = ENV.fetch('DEEPSEEK_API_KEY', 'demo-key')
      config.mistral_api_key = ENV.fetch('MISTRAL_API_KEY', 'demo-key')
      config.openrouter_api_key = ENV.fetch('OPENROUTER_API_KEY', 'demo-key')
      config.default_model = 'gpt-3.5-turbo'
      config.default_embedding_model = 'text-embedding-3-large'
      config.default_image_model = 'dall-e-3'
    end
  end

  def self.configure_for_environment(env)
    case env
    when 'development'
      RubyLLM.configure do |config|
        config.use_new_acts_as = true
        config.request_timeout = 30
        config.max_retries = 1
        config.openai_api_key = ENV.fetch('OPENAI_API_KEY', 'demo-key')
        config.default_model = 'gpt-3.5-turbo'
      end
    when 'production'
      RubyLLM.configure do |config|
        config.use_new_acts_as = true
        config.request_timeout = 180
        config.max_retries = 5
        config.openai_api_key = ENV.fetch('OPENAI_API_KEY', 'demo-key')
        config.default_model = 'gpt-4'
      end
    end
  end

  def self.configure_with_middleware
    RubyLLM.configure do |config|
      config.use_new_acts_as = true
      config.request_timeout = 120
      config.max_retries = 3
      config.openai_api_key = ENV.fetch('OPENAI_API_KEY', 'demo-key')
      config.default_model = 'gpt-3.5-turbo'

      # –≠–º—É–ª—è—Ü–∏—è middleware
      config.before_request = lambda { |req| puts "Request: #{req.model}" }
      config.after_request = lambda { |res| puts "Response received" }
      config.on_error = lambda { |err, req| puts "Error: #{err.message}" }
    end
  end

  def self.configure_with_error_handling
    RubyLLM.configure do |config|
      config.use_new_acts_as = true
      config.request_timeout = 60
      config.max_retries = 2
      config.openai_api_key = ENV.fetch('OPENAI_API_KEY', 'demo-key')
      config.anthropic_api_key = ENV.fetch('ANTHROPIC_API_KEY', 'demo-key')
      config.gemini_api_key = ENV.fetch('GEMINI_API_KEY', 'demo-key')
      config.default_model = 'gpt-3.5-turbo'

      config.on_error = lambda { |err, req| puts "Error handled: #{err.message}" }
    end
  end

  def self.configure_rails_integration
    # –≠–º—É–ª—è—Ü–∏—è anyway_config
    app_config = OpenStruct.new(
      llm_model: 'gpt-3.5-turbo',
      llm_temperature: 0.7,
      llm_max_tokens: 1000,
      llm_request_timeout: 120,
      llm_max_retries: 2,
      llm_system_prompt: 'You are a helpful assistant.',
      openai_api_key: ENV.fetch('OPENAI_API_KEY', 'demo-key'),
      anthropic_api_key: ENV.fetch('ANTHROPIC_API_KEY', 'demo-key'),
      gemini_api_key: ENV.fetch('GEMINI_API_KEY', 'demo-key')
    )

    RubyLLM.configure do |config|
      config.use_new_acts_as = true
      config.request_timeout = app_config.llm_request_timeout
      config.max_retries = app_config.llm_max_retries
      config.openai_api_key = app_config.openai_api_key
      config.anthropic_api_key = app_config.anthropic_api_key
      config.gemini_api_key = app_config.gemini_api_key
      config.default_model = app_config.llm_model
      config.default_embedding_model = 'text-embedding-3-large'
      config.default_image_model = 'dall-e-3'
    end
  end
end

# –ö–ª–∞—Å—Å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ runtime –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
class RuntimeConfigurationDemo
  def self.show_dynamic_model_selection
    puts "\nüîÑ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π"
    puts "-" * 45

    # –§–∞–±—Ä–∏–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏
    class ModelSelector
      MODELS = {
        code_generation: 'gpt-4',
        creative_writing: 'claude-sonnet-4',
        quick_response: 'gpt-3.5-turbo',
        multilingual: 'gemini-pro',
        cost_effective: 'deepseek-chat'
      }.freeze

      def self.for_task(task_type)
        model = MODELS[task_type] || 'gpt-3.5-turbo'
        puts "   –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å '#{model}' –¥–ª—è –∑–∞–¥–∞—á–∏ '#{task_type}'"
        model
      end

      def self.available_models
        MODELS.keys
      end
    end

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    tasks = [:code_generation, :creative_writing, :quick_response, :multilingual, :cost_effective]

    tasks.each do |task|
      model = ModelSelector.for_task(task)
      puts "   ‚úì –ó–∞–¥–∞—á–∞: #{task} ‚Üí –ú–æ–¥–µ–ª—å: #{model}"
    end

    puts "\n‚úÖ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π —Ä–∞–±–æ—Ç–∞–µ—Ç"
  end

  def self.show_context_aware_configuration
    puts "\nüéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"
    puts "-" * 50

    class ContextAwareLLM
      def initialize(user_id)
        @user_id = user_id
      end

      def chat(message, context = {})
        model = select_model_for_context(context)
        system_prompt = build_system_prompt(context)

        puts "   –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: #{@user_id}"
        puts "   –ö–æ–Ω—Ç–µ–∫—Å—Ç: #{context}"
        puts "   –ú–æ–¥–µ–ª—å: #{model}"
        puts "   System prompt: #{system_prompt.truncate(50)}..."

        # –ó–¥–µ—Å—å –±—ã–ª –±—ã —Ä–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ RubyLLM
        "–û—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ #{model}"
      end

      private

      def select_model_for_context(context)
        case context[:task]
        when :coding
          context[:senior] ? 'gpt-4' : 'gpt-3.5-turbo'
        when :creative
          'claude-sonnet-4'
        when :analysis
          'gpt-4'
        else
          'gpt-3.5-turbo'
        end
      end

      def build_system_prompt(context)
        base_prompt = "–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."

        case context[:task]
        when :coding
          base_prompt += " –¢—ã - –æ–ø—ã—Ç–Ω—ã–π Ruby —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫."
        when :creative
          base_prompt += " –¢—ã - –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –ø–∏—Å–∞—Ç–µ–ª—å."
        when :analysis
          base_prompt += " –¢—ã - –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö."
        end

        base_prompt
      end
    end

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    llm = ContextAwareLLM.new('user_123')

    contexts = [
      { task: :coding, senior: true },
      { task: :creative },
      { task: :analysis, domain: 'business' },
      {}
    ]

    contexts.each do |context|
      llm.chat("–ü—Ä–∏–º–µ—Ä —Å–æ–æ–±—â–µ–Ω–∏—è", context)
      puts
    end

    puts "‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç"
  end
end

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
if __FILE__ == $0
  # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
  RubyLLMConfigurationExamples.run_all_examples

  # –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π runtime –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
  RuntimeConfigurationDemo.show_dynamic_model_selection
  RuntimeConfigurationDemo.show_context_aware_configuration

  puts "\nüéâ –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!"
  puts "\nüí° –°–æ–≤–µ—Ç—ã:"
  puts "   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è API –∫–ª—é—á–µ–π"
  puts "   ‚Ä¢ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π"
  puts "   ‚Ä¢ –í–Ω–µ–¥—Ä—è–π—Ç–µ retry –ª–æ–≥–∏–∫—É –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫"
  puts "   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ anyway_config –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"
  puts "   ‚Ä¢ –í—ã–±–∏—Ä–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏"
end